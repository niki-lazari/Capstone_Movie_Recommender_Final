{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "988505f8-709c-4b63-af2e-88355088bd6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q \"huggingface_hub[hf_xet]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b03ce97b-4a59-403f-a78e-6c8dc954c8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ART set to: C:\\Users\\kylek\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# ===== Master v3 ‚Äî Cell 1: Imports & Key Detection (fixed) =====\n",
    "import os, re, ast, json, pickle, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "from tensorflow import keras\n",
    "\n",
    "ART = Path(r\"C:\\Users\\kylek\\artifacts\").resolve()\n",
    "ART.mkdir(parents=True, exist_ok=True)\n",
    "print(\"ART set to:\", ART)\n",
    "\n",
    "#assert (ART/\"faiss_index.bin\").exists(), \"Missing FAISS index\"\n",
    "#assert (ART/\"movie_embeddings.npy\").exists(), \"Missing embeddings\"\n",
    "#assert (ART/\"movie_ids.npy\").exists(), \"Missing movie_ids.npy\"\n",
    "#assert (ART/\"svd_model.pkl\").exists() and (ART/\"ncf_model.keras\").exists(), \"Missing CF models\"\n",
    "\n",
    "# Load embeddings & FAISS\n",
    "#emb = np.load(ART/\"movie_embeddings.npy\")\n",
    "#emb_ids = np.load(ART/\"movie_ids.npy\", allow_pickle=True).tolist()\n",
    "#faiss_index = faiss.read_index(str(ART/\"faiss_index.bin\"))\n",
    "\n",
    "def norm_title(s: str) -> str:\n",
    "    s = (str(s) or \"\").lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)   # <-- fixed: pass 's' as third arg\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "#def is_intable(x):\n",
    "    #try:\n",
    "        #int(str(x))\n",
    "        #return True\n",
    "    #except Exception:\n",
    "        #return False\n",
    "\n",
    "#numeric_count = sum(is_intable(x) for x in emb_ids)\n",
    "#TITLE_MODE = numeric_count < 0.5 * len(emb_ids)\n",
    "\n",
    "#emb_keys = [norm_title(x) for x in emb_ids] if TITLE_MODE else [int(str(x)) for x in emb_ids]\n",
    "#key2idx = {k: i for i, k in enumerate(emb_keys)}\n",
    "#key_set = set(key2idx.keys())\n",
    "\n",
    "#print(f\"‚úÖ Emb: {emb.shape}, FAISS={faiss_index.ntotal}, key_mode={'title' if TITLE_MODE else 'numeric'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a88920a6-ddd1-4d8c-9049-bbeb415371eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CF loaded | users=330,712 movies=97,170\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 2: CF models & ID maps =====\n",
    "import json, pickle\n",
    "from tensorflow import keras\n",
    "\n",
    "with open(ART/\"svd_model.pkl\",\"rb\") as f:\n",
    "    svd_model = pickle.load(f)\n",
    "\n",
    "ncf_model = keras.models.load_model(ART/\"ncf_model.keras\")\n",
    "\n",
    "with open(ART/\"user_to_idx.json\") as f:\n",
    "    user_to_idx = json.load(f)\n",
    "\n",
    "with open(ART/\"movie_to_idx.json\") as f:\n",
    "    movie_to_idx = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ CF loaded | users={len(user_to_idx):,} movies={len(movie_to_idx):,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85a09610-8e2f-4a53-807a-96a027892271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ TMDB loaded: 43,858 rows\n",
      "üé≠ genre_names non-empty: 39985\n",
      "‚öñÔ∏è weighted_rating ready; sample: 6.06\n",
      "üé≠ themes merged: 16259\n",
      "üß† sentiments merged (happy non-null): 17903\n",
      "lead_gender non-null: 0\n",
      "üîé sample: Percy Jackson & the Olympians: The Lightning Thief (2010) | WR=6.06\n"
     ]
    }
   ],
   "source": [
    "# ===== Cell 3: TMDB + Themes + Sentiments + WR + lead_gender =====\n",
    "import ast, numpy as np, pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Load TMDB\n",
    "tmdb = pd.read_parquet(\"tmdb_fully_enriched.parquet\")\n",
    "tmdb[\"title_norm\"] = tmdb[\"tmdb_title\"].apply(norm_title)\n",
    "print(f\"‚úÖ TMDB loaded: {len(tmdb):,} rows\")\n",
    "\n",
    "# Robust genre names\n",
    "GENRE_MAPPING = {\n",
    "    28:\"Action\",12:\"Adventure\",16:\"Animation\",35:\"Comedy\",80:\"Crime\",99:\"Documentary\",\n",
    "    18:\"Drama\",10751:\"Family\",14:\"Fantasy\",36:\"History\",27:\"Horror\",10402:\"Music\",\n",
    "    9648:\"Mystery\",10749:\"Romance\",878:\"Science Fiction\",10770:\"TV Movie\",53:\"Thriller\",\n",
    "    10752:\"War\",37:\"Western\"\n",
    "}\n",
    "def to_genre_names(g):\n",
    "    if isinstance(g,(list,np.ndarray)):\n",
    "        ids=list(g)\n",
    "    elif isinstance(g,str):\n",
    "        try:\n",
    "            p=ast.literal_eval(g)\n",
    "            ids=list(p) if isinstance(p,(list,tuple,np.ndarray)) else []\n",
    "        except Exception:\n",
    "            ids=[]\n",
    "    else:\n",
    "        ids=[]\n",
    "    out=[]\n",
    "    for x in ids:\n",
    "        try:\n",
    "            name=GENRE_MAPPING.get(int(x))\n",
    "            if name: out.append(name)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return out\n",
    "\n",
    "tmdb[\"genre_names\"] = tmdb[\"genre_ids\"].apply(to_genre_names)\n",
    "print(\"üé≠ genre_names non-empty:\", (tmdb[\"genre_names\"].str.len()>0).sum())\n",
    "\n",
    "# Weighted rating (IMDB-style)\n",
    "if \"weighted_rating\" not in tmdb.columns:\n",
    "    C = tmdb[\"vote_average\"].mean()\n",
    "    m = 1000\n",
    "    def wr(row, C=C, m=m):\n",
    "        v = float(row.get(\"vote_count\",0) or 0)\n",
    "        R = float(row.get(\"vote_average\",C) or C)\n",
    "        return (v/(v+m))*R + (m/(v+m))*C\n",
    "    tmdb[\"weighted_rating\"] = tmdb.apply(wr, axis=1)\n",
    "print(\"‚öñÔ∏è weighted_rating ready; sample:\", round(float(tmdb[\"weighted_rating\"].iloc[0]), 2))\n",
    "\n",
    "# Themes (LDA over RT)\n",
    "with open(ART/\"movie_themes.pkl\",\"rb\") as f:\n",
    "    theme_art = pickle.load(f)\n",
    "theme_df = pd.DataFrame({\n",
    "    \"title_norm\": [norm_title(t) for t in theme_art[\"movie_titles\"]],\n",
    "    \"lda_themes\": theme_art[\"themes\"]\n",
    "})\n",
    "tmdb = tmdb.merge(theme_df, on=\"title_norm\", how=\"left\")\n",
    "tmdb[\"lda_themes\"] = tmdb[\"lda_themes\"].apply(\n",
    "    lambda x: x if isinstance(x, list) else ([] if pd.isna(x) else [x])\n",
    ")\n",
    "print(\"üé≠ themes merged:\", tmdb[\"lda_themes\"].map(len).gt(0).sum())\n",
    "\n",
    "# Sentiments (movie-level BERT aggregation)\n",
    "sent = pd.read_pickle(ART/\"movie_sentiments.pkl\")\n",
    "sent[\"title_norm\"] = sent[\"movie_title\"].apply(norm_title)\n",
    "emo_cols = [c for c in sent.columns if c not in [\"movie_title\",\"rotten_tomatoes_link\",\"title_norm\"]]\n",
    "tmdb = tmdb.merge(sent[[\"title_norm\"]+emo_cols], on=\"title_norm\", how=\"left\")\n",
    "print(\"üß† sentiments merged (happy non-null):\", tmdb[\"sentiment_happy\"].notna().sum())\n",
    "\n",
    "# lead_gender from top-billed cast if available\n",
    "import ast\n",
    "\n",
    "def _as_list(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if isinstance(x, str):\n",
    "        try: return ast.literal_eval(x)\n",
    "        except Exception: return []\n",
    "    return x if isinstance(x, list) else []\n",
    "\n",
    "def lead_gender(row):\n",
    "    c = _as_list(row.get(\"cast\", []))\n",
    "    if isinstance(c, list) and c and isinstance(c[0], dict):\n",
    "        g = c[0].get(\"gender\")\n",
    "        return \"female\" if g == 1 else (\"male\" if g == 2 else None)\n",
    "    return None\n",
    "\n",
    "tmdb[\"lead_gender\"] = tmdb.apply(lead_gender, axis=1)\n",
    "print(\"lead_gender non-null:\", tmdb[\"lead_gender\"].notna().sum())\n",
    "\n",
    "# Quick preview\n",
    "sample = tmdb.iloc[0]\n",
    "print(f\"üîé sample: {sample['tmdb_title']} ({sample.get('year')}) | WR={round(float(sample['weighted_rating']),2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "167270ad-c885-46ce-92d6-fa47ea44fde0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0979000aac434c14897ae95dfbf01d18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/723 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ FAISS rebuilt on TMDB: 46229 vectors; tmdb=46229\n"
     ]
    }
   ],
   "source": [
    "# === Rebuild FAISS on current TMDB catalog (aligned keys) ===\n",
    "import numpy as np, faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# 1) Keys aligned to TMDB\n",
    "tmdb = tmdb.copy()\n",
    "tmdb[\"__key__\"] = tmdb[\"title_norm\"].astype(str)\n",
    "\n",
    "# 2) Embed Title + Overview + Genres\n",
    "def _concat_text(row):\n",
    "    g = \" \".join(row.get(\"genre_names\", [])) if isinstance(row.get(\"genre_names\"), list) else \"\"\n",
    "    return f\"{row.get('tmdb_title','')} {row.get('overview','')} {g}\".strip()\n",
    "\n",
    "texts = tmdb.apply(_concat_text, axis=1).fillna(\"\").str.slice(0, 700).tolist()\n",
    "\n",
    "# 3) Encode & normalize (cosine similarity via inner product)\n",
    "encoder = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "emb = encoder.encode(\n",
    "    texts, batch_size=64, show_progress_bar=True, normalize_embeddings=True\n",
    ").astype(\"float32\")\n",
    "\n",
    "# 4) Build FAISS index\n",
    "d = emb.shape[1]\n",
    "faiss_index = faiss.IndexFlatIP(d)\n",
    "faiss_index.add(emb)\n",
    "\n",
    "# 5) Lookup structures aligned with TMDB\n",
    "emb_keys = tmdb[\"__key__\"].tolist()\n",
    "key2idx = {k: i for i, k in enumerate(emb_keys)}\n",
    "key_set = set(emb_keys)\n",
    "\n",
    "print(f\"‚úÖ FAISS rebuilt on TMDB: {faiss_index.ntotal} vectors; tmdb={len(tmdb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c245161-a1bd-4235-a640-9392156e8384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key overlap with TMDB (~100% expected): 90.1%\n"
     ]
    }
   ],
   "source": [
    "overlap = len(set(emb_keys) & set(tmdb[\"title_norm\"])) / max(1, len(tmdb))\n",
    "print(f\"Key overlap with TMDB (~100% expected): {overlap:.1%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "49437de2-827d-4f92-a52c-ea4e50d677ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ cast normalized: 1.0  (fraction of rows with list)\n"
     ]
    }
   ],
   "source": [
    "# === Normalize `cast` to list[str] for reliable actor filtering ===\n",
    "import ast\n",
    "\n",
    "def _cast_to_list(v):\n",
    "    # Already a list\n",
    "    if isinstance(v, list):\n",
    "        return [str(x).strip() for x in v if str(x).strip()]\n",
    "    # String forms: JSON-like [\"A\",\"B\"], pipe- or comma-separated, or single name\n",
    "    if isinstance(v, str):\n",
    "        s = v.strip()\n",
    "        if not s:\n",
    "            return []\n",
    "        # JSON/list-as-string\n",
    "        if (s.startswith(\"[\") and s.endswith(\"]\")) or (s.startswith(\"(\") and s.endswith(\")\")):\n",
    "            try:\n",
    "                parsed = ast.literal_eval(s)\n",
    "                if isinstance(parsed, (list, tuple)):\n",
    "                    return [str(x).strip() for x in parsed if str(x).strip()]\n",
    "            except Exception:\n",
    "                pass\n",
    "        # Pipe or comma separated fallback\n",
    "        if \"|\" in s:\n",
    "            return [x.strip() for x in s.split(\"|\") if x.strip()]\n",
    "        if \",\" in s:\n",
    "            return [x.strip() for x in s.split(\",\") if x.strip()]\n",
    "        # Single name as last resort\n",
    "        return [s]\n",
    "    # Anything else (NaN, None, etc.)\n",
    "    return []\n",
    "\n",
    "if \"cast\" in tmdb.columns:\n",
    "    tmdb[\"cast\"] = tmdb[\"cast\"].apply(_cast_to_list)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è `cast` column not found in TMDB; actor filtering will be skipped.\")\n",
    "\n",
    "# quick sanity\n",
    "print(\"‚úÖ cast normalized:\",\n",
    "      tmdb[\"cast\"].map(lambda x: isinstance(x, list)).mean(),\n",
    "      \" (fraction of rows with list)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f6515a0-2273-4d44-9655-25ace2099400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tmdb_title</th>\n",
       "      <th>year</th>\n",
       "      <th>cast</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [tmdb_title, year, cast]\n",
       "Index: []"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Should be > 0 for well-known actors\n",
    "tmdb[tmdb[\"cast\"].apply(lambda L: any(\"bill murray\" in n.lower() for n in L))].head()[[\"tmdb_title\",\"year\",\"cast\"]].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c5c3b5d-14ba-45b1-a72f-d672600cfbb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "UNIFIED ZERO-SHOT TAGGING ‚Äî COMPLETE LABEL SET\n",
      "================================================================================\n",
      "\n",
      "üîß Initializing zero-shot classifier...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Classifier loaded: facebook/bart-large-mnli\n",
      "\n",
      "üìã Total labels in unified set: 104\n",
      "   Emotional/Tonal: ~23\n",
      "   Subject/Genre: ~33\n",
      "   Character Types: ~10\n",
      "   Relationships: ~7\n",
      "   Themes/Journey: ~13\n",
      "   Pacing/Setting: ~11\n",
      "\n",
      "üìö Loading reviews...\n",
      "   Loaded 976,093 reviews\n",
      "   Aggregated into 17,023 movies\n",
      "   Avg review length: 7878 chars\n",
      "\n",
      "‚ôªÔ∏è  Found 532 existing shards\n",
      "‚úÖ Already tagged: 17,023 movies\n",
      "\n",
      "üéØ TAGGING PLAN:\n",
      "   Total movies: 17,023\n",
      "   Already done: 17,023\n",
      "   Remaining: 0\n",
      "   Est. time: ~0.0 hours (at 4.5 sec/movie)\n",
      "\n",
      "‚úÖ All movies already tagged!\n",
      "\n",
      "================================================================================\n",
      "üì¶ CONSOLIDATING SHARDS...\n",
      "Loading 532 shard files...\n",
      "‚úÖ Loaded 17,023 unique movies\n",
      "üíæ Saved master checkpoint: zs_unified_checkpoint.parquet\n"
     ]
    }
   ],
   "source": [
    "# ===== UNIFIED Zero-Shot Tagging ‚Äî Complete Label Set =====\n",
    "# Tags ALL movies with: emotional + subject + character + relationship + theme labels\n",
    "# Optimized for multi-core processing, resumable sharding\n",
    "# Saves to: zs_unified_checkpoint.parquet (new file, won't overwrite old checkpoints)\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Force Transformers to ignore TF/Keras\n",
    "os.environ[\"TRANSFORMERS_NO_TF\"] = \"1\"\n",
    "os.environ[\"HF_HUB_DISABLE_TELEMETRY\"] = \"1\"\n",
    "\n",
    "# Ensure PyTorch is available\n",
    "try:\n",
    "    import torch\n",
    "except Exception as e:\n",
    "    raise RuntimeError(\"PyTorch is not installed. Install a CPU build of torch and rerun this cell.\") from e\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"UNIFIED ZERO-SHOT TAGGING ‚Äî COMPLETE LABEL SET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# --- Build zero-shot classifier (PyTorch backend ONLY)\n",
    "print(\"\\nüîß Initializing zero-shot classifier...\")\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    task=\"zero-shot-classification\",\n",
    "    model=\"facebook/bart-large-mnli\",\n",
    "    framework=\"pt\",  # PyTorch only\n",
    "    device=-1        # CPU (set to 0 for CUDA GPU if you have one)\n",
    ")\n",
    "print(\"‚úÖ Classifier loaded: facebook/bart-large-mnli\\n\")\n",
    "\n",
    "# --- Define norm_title function\n",
    "def norm_title(s: str) -> str:\n",
    "    s = (str(s) or \"\").lower()\n",
    "    s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
    "    return \" \".join(s.split())\n",
    "\n",
    "# --- COMPLETE UNIFIED LABEL SET (~80 labels) ---\n",
    "UNIFIED_LABELS = [\n",
    "    # === EMOTIONAL / TONAL (from Cell 3.5) ===\n",
    "    \"inspiring\", \"uplifting\", \"comforting\", \"feel-good\", \"family friendly\",\n",
    "    \"family values\", \"tragic\", \"heartbreaking\", \"bittersweet\", \"lost love\",\n",
    "    \"romantic\", \"slow-burn\", \"dark\", \"bleak\",\n",
    "    \"mystery\", \"suspenseful\", \"thriller\", \"documentary\", \"coming of age\",\n",
    "    \"redemption\", \"friendship\", \"found family\", \"biographical\",\n",
    "    \n",
    "    # === SUBJECT / GENRE (from Cell 3.6) ===\n",
    "    # Crime / thriller / espionage\n",
    "    \"mafia\", \"organized crime\", \"gangster\", \"mob\", \"crime\", \"noir\",\n",
    "    \"heist\", \"detective\", \"police procedural\", \"courtroom\", \"espionage\",\n",
    "    \"political thriller\",\n",
    "    \n",
    "    # Sci-fi / speculative\n",
    "    \"sci-fi\", \"science fiction\", \"cyberpunk\", \"dystopian\", \"post-apocalyptic\",\n",
    "    \"time travel\", \"space opera\", \"alien\", \"robot ai\", \"kaiju\",\n",
    "    \n",
    "    # Horror / creatures\n",
    "    \"vampire\", \"zombie\", \"psychological horror\",\n",
    "    \n",
    "    # Action / martial / western\n",
    "    \"superhero\", \"martial arts\", \"western\",\n",
    "    \n",
    "    # Drama / art\n",
    "    \"period drama\", \"historical epic\", \"surreal\", \"absurdist\", \"existential\", \n",
    "    \"arthouse\", \"satire\", \"parody\",\n",
    "    \n",
    "    # Fantasy\n",
    "    \"fantasy\", \"sword and sorcery\", \"myth and legend\",\n",
    "    \n",
    "    # === CHARACTER TYPES (NEW) ===\n",
    "    \"strong female lead\", \"female protagonist\", \"male protagonist\", \n",
    "    \"ensemble cast\", \"anti-hero\", \"underdog story\", \"mentor relationship\",\n",
    "    \"father-son relationship\", \"mother-daughter relationship\", \"unlikely friendship\",\n",
    "    \n",
    "    # === RELATIONSHIP DYNAMICS (NEW) ===\n",
    "    \"forbidden love\", \"unrequited love\", \"toxic relationship\", \"family drama\",\n",
    "    \"generational conflict\", \"class struggle\", \"cultural clash\",\n",
    "    \n",
    "    # === EMOTIONAL JOURNEY / THEMES (NEW) ===\n",
    "    \"revenge\", \"betrayal\", \"sacrifice\", \"survival\", \"overcoming adversity\",\n",
    "    \"self-discovery\", \"identity crisis\", \"moral dilemma\", \"loss and grief\",\n",
    "    \"hope\", \"justice vs revenge\", \"loneliness\", \"power and corruption\",\n",
    "    \n",
    "    # === PACING / ATMOSPHERE (NEW) ===\n",
    "    \"fast-paced\", \"intense\", \"contemplative\", \"whimsical\", \"gritty\", \n",
    "    \"atmospheric\", \"cerebral\",\n",
    "    \n",
    "    # === SETTING / SCOPE (NEW) ===\n",
    "    \"small-town\", \"isolated setting\", \"road trip\", \"epic scope\", \"intimate story\"\n",
    "]\n",
    "\n",
    "print(f\"üìã Total labels in unified set: {len(UNIFIED_LABELS)}\")\n",
    "print(f\"   Emotional/Tonal: ~23\")\n",
    "print(f\"   Subject/Genre: ~33\")\n",
    "print(f\"   Character Types: ~10\")\n",
    "print(f\"   Relationships: ~7\")\n",
    "print(f\"   Themes/Journey: ~13\")\n",
    "print(f\"   Pacing/Setting: ~11\\n\")\n",
    "\n",
    "# --- Tagging function (optimized)\n",
    "MAX_CHARS = 700  # Sweet spot for speed vs quality\n",
    "\n",
    "def unified_zs_tags(text: str, top_k=3, conf=0.30):\n",
    "    \"\"\"\n",
    "    Tags text with unified label set.\n",
    "    Uses first 350 + last 350 chars to capture setup and resolution.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str) or len(text) < 40:\n",
    "        return []\n",
    "    \n",
    "    # Smart truncation: first half + last half\n",
    "    if len(text) > MAX_CHARS:\n",
    "        mid = MAX_CHARS // 2\n",
    "        txt = text[:mid] + \" \" + text[-mid:]\n",
    "    else:\n",
    "        txt = text\n",
    "    \n",
    "    out = classifier(txt, candidate_labels=UNIFIED_LABELS, multi_label=True)\n",
    "    scored = sorted(zip(out[\"labels\"], out[\"scores\"]), key=lambda x: x[1], reverse=True)\n",
    "    return [lbl for lbl, s in scored[:top_k] if s >= conf]\n",
    "\n",
    "# --- Load and aggregate review text ---\n",
    "print(\"üìö Loading reviews...\")\n",
    "reviews_path = ART / \"reviews_with_emotions.parquet\"\n",
    "assert reviews_path.exists(), f\"Missing {reviews_path}\"\n",
    "\n",
    "reviews_df = pd.read_parquet(reviews_path)\n",
    "reviews_df[\"title_norm\"] = reviews_df[\"movie_title\"].apply(norm_title)\n",
    "\n",
    "print(f\"   Loaded {len(reviews_df):,} reviews\")\n",
    "\n",
    "# Aggregate review text per movie\n",
    "agg = (\n",
    "    reviews_df.groupby(\"title_norm\", as_index=False)[\"review_content\"]\n",
    "    .agg(lambda x: \" \".join(map(str, x)))\n",
    ")\n",
    "\n",
    "print(f\"   Aggregated into {len(agg):,} movies\")\n",
    "print(f\"   Avg review length: {agg['review_content'].str.len().mean():.0f} chars\\n\")\n",
    "\n",
    "# --- Setup unified checkpoint directory ---\n",
    "unified_shard_dir = ART / \"zs_unified_shards\"\n",
    "unified_shard_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load already-done titles from shards\n",
    "done_titles = set()\n",
    "shard_count = 0\n",
    "\n",
    "for p in unified_shard_dir.glob(\"unified_shard_*.parquet\"):\n",
    "    try:\n",
    "        dfp = pd.read_parquet(p, columns=[\"title_norm\"])\n",
    "        done_titles.update(dfp[\"title_norm\"].tolist())\n",
    "        shard_count += 1\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "if shard_count > 0:\n",
    "    print(f\"‚ôªÔ∏è  Found {shard_count} existing shards\")\n",
    "    print(f\"‚úÖ Already tagged: {len(done_titles):,} movies\\n\")\n",
    "\n",
    "# Filter out done titles\n",
    "todo = agg[~agg[\"title_norm\"].isin(done_titles)].reset_index(drop=True)\n",
    "\n",
    "print(f\"üéØ TAGGING PLAN:\")\n",
    "print(f\"   Total movies: {len(agg):,}\")\n",
    "print(f\"   Already done: {len(done_titles):,}\")\n",
    "print(f\"   Remaining: {len(todo):,}\")\n",
    "print(f\"   Est. time: ~{len(todo) * 4.5 / 3600:.1f} hours (at 4.5 sec/movie)\\n\")\n",
    "\n",
    "if len(todo) == 0:\n",
    "    print(\"‚úÖ All movies already tagged!\")\n",
    "else:\n",
    "    # --- Tag in batches with progress tracking ---\n",
    "    BATCH_SIZE = 32  # Optimized for your 12-core Ryzen 9\n",
    "    \n",
    "    shard_num = shard_count  # Continue numbering from existing shards\n",
    "    \n",
    "    print(f\"üè∑Ô∏è  Starting unified tagging...\")\n",
    "    print(f\"   Batch size: {BATCH_SIZE} movies per shard\")\n",
    "    print(f\"   Total batches: {len(todo) // BATCH_SIZE + 1}\\n\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    batch_times = []\n",
    "    \n",
    "    for i in tqdm(range(0, len(todo), BATCH_SIZE), desc=\"Unified tagging\"):\n",
    "        batch_start = time.time()\n",
    "        \n",
    "        chunk = todo.iloc[i:i+BATCH_SIZE]\n",
    "        \n",
    "        # Tag each title in chunk\n",
    "        tags = [unified_zs_tags(t) for t in chunk[\"review_content\"].tolist()]\n",
    "        \n",
    "        # Create shard dataframe\n",
    "        part = pd.DataFrame({\n",
    "            \"title_norm\": chunk[\"title_norm\"].tolist(),\n",
    "            \"unified_tags\": tags\n",
    "        })\n",
    "        \n",
    "        # Write shard\n",
    "        shard_path = unified_shard_dir / f\"unified_shard_{shard_num:06d}.parquet\"\n",
    "        part.to_parquet(shard_path, index=False)\n",
    "        \n",
    "        shard_num += 1\n",
    "        batch_times.append(time.time() - batch_start)\n",
    "        \n",
    "        # Progress update every 25 shards\n",
    "        if shard_num % 25 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            titles_done = min(i + BATCH_SIZE, len(todo))\n",
    "            \n",
    "            # Calculate rate from recent batches\n",
    "            recent_rate = sum(batch_times[-25:]) / len(batch_times[-25:]) / BATCH_SIZE\n",
    "            remaining_titles = len(todo) - titles_done\n",
    "            remaining_time = remaining_titles * recent_rate\n",
    "            \n",
    "            print(f\"\\n  ‚úì Shard {shard_num:06d} | \"\n",
    "                  f\"{titles_done:,}/{len(todo):,} titles ({titles_done/len(todo)*100:.1f}%) | \"\n",
    "                  f\"{1/recent_rate:.1f} titles/sec | \"\n",
    "                  f\"~{remaining_time/60:.0f}min remaining\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"\\n‚úÖ Tagging complete!\")\n",
    "    print(f\"   Time: {total_time/60:.1f} minutes ({total_time/3600:.2f} hours)\")\n",
    "    print(f\"   Average: {total_time/len(todo):.2f} sec/movie\")\n",
    "    print(f\"   New shards written: {shard_num - shard_count}\")\n",
    "\n",
    "# --- CONSOLIDATE ALL SHARDS INTO MASTER CHECKPOINT ---\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üì¶ CONSOLIDATING SHARDS...\")\n",
    "shards = list(unified_shard_dir.glob(\"unified_shard_*.parquet\"))\n",
    "\n",
    "if shards:\n",
    "    print(f\"Loading {len(shards)} shard files...\")\n",
    "    unified_df = (\n",
    "        pd.concat([pd.read_parquet(p) for p in shards], ignore_index=True)\n",
    "          .drop_duplicates(\"title_norm\", keep=\"last\")\n",
    "    )\n",
    "\n",
    "    # --- Normalize unified_tags to real Python lists (prevents 0-tag reporting) ---\n",
    "    import numpy as np, ast\n",
    "    def to_pylist(x):\n",
    "        if isinstance(x, list): return x\n",
    "        if isinstance(x, np.ndarray): return x.tolist()\n",
    "        if x is None: return []\n",
    "        if isinstance(x, str):\n",
    "            try: return ast.literal_eval(x)\n",
    "            except Exception: return [x]\n",
    "        try: return list(x)\n",
    "        except Exception: return []\n",
    "\n",
    "    unified_df[\"unified_tags\"] = unified_df[\"unified_tags\"].apply(to_pylist)\n",
    "    print(f\"‚úÖ Loaded {len(unified_df):,} unique movies\")\n",
    "\n",
    "    # --- Save master checkpoint ---\n",
    "    master_checkpoint = ART / \"zs_unified_checkpoint.parquet\"\n",
    "    unified_df.to_parquet(master_checkpoint, index=False)\n",
    "    print(f\"üíæ Saved master checkpoint: {master_checkpoint.name}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No shards found - nothing to consolidate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc928c30-370c-4fbd-b3ee-747e4c1ff744",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üì¶ CONSOLIDATING V2 ZERO-SHOT TAGS\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'unified_tags'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'unified_tags'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m: \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Normalize V2 tags\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m unified_df_v2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munified_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m unified_df_v2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munified_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(to_pylist)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Loaded \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(unified_df_v2)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m unique movies from V2\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Merge V2 tags into tmdb\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3798\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3793\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3795\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3796\u001b[0m     ):\n\u001b[0;32m   3797\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3803\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'unified_tags'"
     ]
    }
   ],
   "source": [
    "# ===== Cell 26: Merge V2 Zero-Shot Tags (Additional Labels) =====\n",
    "# Same logic as V1 merge (Cell 25), but for the V2 checkpoint\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"üì¶ CONSOLIDATING V2 ZERO-SHOT TAGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load V2 checkpoint\n",
    "master_checkpoint_v2 = ART / \"zs_unified_checkpoint_v2.parquet\"\n",
    "assert master_checkpoint_v2.exists(), f\"Missing {master_checkpoint_v2}\"\n",
    "\n",
    "unified_df_v2 = pd.read_parquet(master_checkpoint_v2)\n",
    "\n",
    "# Use same normalization function as V1\n",
    "def to_pylist(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if isinstance(x, np.ndarray): return x.tolist()\n",
    "    if x is None: return []\n",
    "    if isinstance(x, str):\n",
    "        try: return ast.literal_eval(x)\n",
    "        except Exception: return [x]\n",
    "    try: return list(x)\n",
    "    except Exception: return []\n",
    "\n",
    "# Normalize V2 tags\n",
    "unified_df_v2[\"unified_tags\"] = unified_df_v2[\"unified_tags\"].apply(to_pylist)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(unified_df_v2):,} unique movies from V2\")\n",
    "\n",
    "# Merge V2 tags into tmdb\n",
    "tmdb = tmdb.merge(unified_df_v2[[\"title_norm\",\"unified_tags\"]], on=\"title_norm\", how=\"left\", suffixes=('', '_v2'))\n",
    "\n",
    "# Union existing review_tags (which has V1) with V2 tags\n",
    "def union_tags(a, b):\n",
    "    A = to_pylist(a)\n",
    "    B = to_pylist(b)\n",
    "    return sorted({str(t).strip() for t in A+B})\n",
    "\n",
    "tmdb[\"review_tags\"] = tmdb.apply(\n",
    "    lambda r: union_tags(r.get(\"review_tags\"), r.get(\"unified_tags_v2\")), \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Drop temporary V2 column\n",
    "tmdb = tmdb.drop(columns=[\"unified_tags_v2\"], errors=\"ignore\")\n",
    "\n",
    "# Validate merged results\n",
    "n_with_tags = tmdb[\"review_tags\"].apply(len).gt(0).sum()\n",
    "avg_tags = tmdb[\"review_tags\"].apply(len).mean()\n",
    "\n",
    "print(f\"‚úÖ V2 tags merged successfully\")\n",
    "print(f\"   Movies with tags: {n_with_tags:,}\")\n",
    "print(f\"   Average tags per movie: {avg_tags:.2f}\")\n",
    "\n",
    "# Get total unique tags across both V1 and V2\n",
    "all_tags = []\n",
    "for tags in tmdb[\"review_tags\"]:\n",
    "    all_tags.extend(tags)\n",
    "unique_tags = len(set(all_tags))\n",
    "\n",
    "print(f\"   Total unique tags (V1 + V2): {unique_tags}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e18bde-4198-47c6-9a66-eeda1fa33e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Consolidate unified checkpoint -> tmdb.review_tags (stand-alone) =====\n",
    "import pandas as pd, numpy as np, ast\n",
    "from pathlib import Path\n",
    "\n",
    "master_checkpoint = ART / \"zs_unified_checkpoint.parquet\"\n",
    "assert master_checkpoint.exists(), f\"Missing {master_checkpoint}\"\n",
    "\n",
    "unified_df = pd.read_parquet(master_checkpoint)\n",
    "\n",
    "def to_pylist(x):\n",
    "    if isinstance(x, list): return x\n",
    "    if isinstance(x, np.ndarray): return x.tolist()\n",
    "    if x is None: return []\n",
    "    if isinstance(x, str):\n",
    "        try: return ast.literal_eval(x)\n",
    "        except Exception: return [x]\n",
    "    try: return list(x)\n",
    "    except Exception: return []\n",
    "\n",
    "unified_df[\"unified_tags\"] = unified_df[\"unified_tags\"].apply(to_pylist)\n",
    "\n",
    "# robust union of existing review_tags (LDA/earlier) with unified tags\n",
    "def union_tags(a, b):\n",
    "    A = to_pylist(a); B = to_pylist(b)\n",
    "    return sorted({str(t).strip() for t in A+B})\n",
    "\n",
    "tmdb = tmdb.merge(unified_df[[\"title_norm\",\"unified_tags\"]], on=\"title_norm\", how=\"left\")\n",
    "tmdb[\"review_tags\"] = tmdb.apply(lambda r: union_tags(r.get(\"review_tags\"), r.get(\"unified_tags\")), axis=1)\n",
    "tmdb = tmdb.drop(columns=[\"unified_tags\"], errors=\"ignore\")\n",
    "\n",
    "print(\"Movies with review_tags:\", tmdb[\"review_tags\"].apply(len).gt(0).sum())\n",
    "print(\"Avg tags per movie:\", tmdb[\"review_tags\"].apply(len).mean())\n",
    "\n",
    "# (optional) persist ready-to-load dataset\n",
    "tmdb.to_parquet(ART / \"tmdb_with_review_tags.parquet\", index=False)\n",
    "print(\"üíæ saved:\", ART / \"tmdb_with_review_tags.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718ed88a-d498-451b-80cf-c19a89cda1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CONSOLIDATED Tag Normalization (replaces Cells 7, 8, 9) =====\n",
    "# This single cell replaces the multiple attempts at normalizing review_tags\n",
    "\n",
    "import ast\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def robust_to_list(x):\n",
    "    \"\"\"\n",
    "    Convert review_tags to Python list regardless of input format.\n",
    "    Handles: lists, None, NaN, numpy arrays, stringified lists, single values\n",
    "    \"\"\"\n",
    "    # Already a proper Python list\n",
    "    if isinstance(x, list):\n",
    "        return x\n",
    "    \n",
    "    # None or NaN (empty)\n",
    "    if x is None or (isinstance(x, float) and pd.isna(x)):\n",
    "        return []\n",
    "    \n",
    "    # NumPy array (from parquet)\n",
    "    if isinstance(x, np.ndarray):\n",
    "        return x.tolist()\n",
    "    \n",
    "    # String representation of a list (e.g., \"['tag1', 'tag2']\")\n",
    "    if isinstance(x, str):\n",
    "        try:\n",
    "            parsed = ast.literal_eval(x)\n",
    "            if isinstance(parsed, (list, tuple)):\n",
    "                return list(parsed)\n",
    "            else:\n",
    "                return [parsed]  # Single item in string form\n",
    "        except (ValueError, SyntaxError):\n",
    "            # If literal_eval fails, treat the whole string as a single tag\n",
    "            return [x] if x.strip() else []\n",
    "    \n",
    "    # Try generic conversion to list (handles tuples, sets, etc.)\n",
    "    try:\n",
    "        return list(x)\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "# Apply normalization ONCE\n",
    "print(\"üîÑ Normalizing review_tags...\")\n",
    "tmdb[\"review_tags\"] = tmdb[\"review_tags\"].apply(robust_to_list)\n",
    "\n",
    "# Validate\n",
    "n_with_tags = (tmdb[\"review_tags\"].apply(len) > 0).sum()\n",
    "avg_tags = tmdb[\"review_tags\"].apply(len).mean()\n",
    "\n",
    "print(f\"‚úÖ review_tags normalized successfully\")\n",
    "print(f\"   Movies with tags: {n_with_tags:,}\")\n",
    "print(f\"   Average tags per movie: {avg_tags:.2f}\")\n",
    "\n",
    "# Sample check\n",
    "sample_tagged = tmdb[tmdb[\"review_tags\"].apply(len) > 0].sample(min(5, n_with_tags))\n",
    "print(f\"\\nüìù Sample tagged movies:\")\n",
    "for _, row in sample_tagged.iterrows():\n",
    "    title = row.get(\"tmdb_title\", \"Unknown\")[:40]\n",
    "    tags = row[\"review_tags\"][:3]\n",
    "    print(f\"  {title:40s} ‚Üí {tags}\")\n",
    "\n",
    "# ===== GENRE FALLBACK TAGS for movies without reviews =====\n",
    "print(\"\\nüéØ Adding genre-based fallback tags for movies without review tags...\")\n",
    "\n",
    "def genre_fallback_tags(row):\n",
    "    \"\"\"Give movies without review tags some basic tags from their genres.\"\"\"\n",
    "    # Only apply if movie has NO review tags\n",
    "    if len(row.get(\"review_tags\", [])) > 0:\n",
    "        return []\n",
    "    \n",
    "    genres = row.get(\"genre_names\", [])\n",
    "    if not genres:\n",
    "        return []\n",
    "    \n",
    "    # Map genres to relevant tags\n",
    "    genre_to_tags = {\n",
    "        \"Action\": [\"action\", \"fast-paced\", \"intense\"],\n",
    "        \"Adventure\": [\"epic scope\"],\n",
    "        \"Animation\": [\"family friendly\", \"feel-good\"],\n",
    "        \"Comedy\": [\"feel-good\", \"uplifting\"],\n",
    "        \"Crime\": [\"crime\", \"gritty\", \"dark\"],\n",
    "        \"Documentary\": [\"documentary\"],\n",
    "        \"Drama\": [\"intense\", \"contemplative\"],\n",
    "        \"Family\": [\"family friendly\", \"feel-good\", \"uplifting\"],\n",
    "        \"Fantasy\": [\"fantasy\", \"epic scope\"],\n",
    "        \"History\": [\"historical epic\", \"period drama\"],\n",
    "        \"Horror\": [\"horror\", \"dark\", \"suspenseful\"],\n",
    "        \"Music\": [\"uplifting\", \"inspiring\"],\n",
    "        \"Mystery\": [\"mystery\", \"suspenseful\"],\n",
    "        \"Romance\": [\"romantic\", \"bittersweet\"],\n",
    "        \"Science Fiction\": [\"sci-fi\"],\n",
    "        \"Thriller\": [\"thriller\", \"suspenseful\", \"intense\"],\n",
    "        \"War\": [\"dark\", \"intense\", \"historical epic\"],\n",
    "        \"Western\": [\"western\", \"gritty\"]\n",
    "    }\n",
    "    \n",
    "    tags = []\n",
    "    for g in genres:\n",
    "        tags.extend(genre_to_tags.get(g, []))\n",
    "    \n",
    "    # Return max 3 unique tags\n",
    "    return list(set(tags))[:3]\n",
    "\n",
    "# Create fallback tags\n",
    "tmdb[\"fallback_tags\"] = tmdb.apply(genre_fallback_tags, axis=1)\n",
    "\n",
    "# Union fallback tags with existing review_tags\n",
    "def union_all_tags(review_tags, fallback_tags):\n",
    "    A = robust_to_list(review_tags)\n",
    "    B = robust_to_list(fallback_tags)\n",
    "    return sorted(set(str(t).strip() for t in A + B))\n",
    "\n",
    "tmdb[\"review_tags\"] = tmdb.apply(\n",
    "    lambda r: union_all_tags(r.get(\"review_tags\"), r.get(\"fallback_tags\")),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Clean up temporary column\n",
    "tmdb = tmdb.drop(columns=[\"fallback_tags\"], errors=\"ignore\")\n",
    "\n",
    "# Add to end of Cell 13, BEFORE the final validation:\n",
    "\n",
    "print(\"\\nüîß Adding manual override tags for famous movies...\")\n",
    "\n",
    "# Sports movies from genres\n",
    "sports_genres = tmdb[\"genre_names\"].apply(lambda g: any(x in str(g).lower() for x in [\"sport\"]))\n",
    "tmdb.loc[sports_genres, \"review_tags\"] = tmdb.loc[sports_genres, \"review_tags\"].apply(\n",
    "    lambda tags: list(set(tags + [\"sports\"]))\n",
    ")\n",
    "\n",
    "# Check if keywords exist\n",
    "if \"keywords\" in tmdb.columns:\n",
    "    print(\"   Using TMDB keywords...\")\n",
    "    # Extract keywords into tags\n",
    "    # (would need to see your keywords column format)\n",
    "\n",
    "print(f\"‚úÖ Override tags added\")\n",
    "\n",
    "# Re-validate\n",
    "n_with_tags_after = (tmdb[\"review_tags\"].apply(len) > 0).sum()\n",
    "avg_tags_after = tmdb[\"review_tags\"].apply(len).mean()\n",
    "\n",
    "print(f\"‚úÖ After genre fallback:\")\n",
    "print(f\"   Movies with tags: {n_with_tags_after:,} (was {n_with_tags:,})\")\n",
    "print(f\"   Average tags: {avg_tags_after:.2f} (was {avg_tags:.2f})\")\n",
    "print(f\"   Coverage: {n_with_tags_after/len(tmdb)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf18857-2443-436d-84a1-ca67c14cedb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Unify tags across sources + compute global rarity counts ===\n",
    "from collections import Counter\n",
    "\n",
    "# 1) Helper to coerce any column to clean list[str]\n",
    "def _to_list(x):\n",
    "    if isinstance(x, list):\n",
    "        return [str(t).strip().lower() for t in x if str(t).strip()]\n",
    "    if isinstance(x, str):\n",
    "        # allow comma- or pipe-separated strings\n",
    "        parts = [p.strip().lower() for p in x.replace(\"|\", \",\").split(\",\")]\n",
    "        return [p for p in parts if p]\n",
    "    return []\n",
    "\n",
    "# 2) (Optional) synonym smoothing you already maintain in Master V4\n",
    "THEME_SYNONYMS = {\n",
    "    \"kung fu\": \"martial arts\",\n",
    "    \"ancient\": \"historical epic\",\n",
    "    \"breakup\": \"lost love\",\n",
    "    # ‚Üê keep/add your existing mappings here\n",
    "}\n",
    "\n",
    "def _apply_synonyms(tags):\n",
    "    out = []\n",
    "    for t in tags:\n",
    "        out.append(THEME_SYNONYMS.get(t, t))\n",
    "    return out\n",
    "\n",
    "# 3) Build a single 'all_tags' column per movie (set-union across sources)\n",
    "tag_cols = []\n",
    "for col in [\"review_tags\", \"lda_themes\", \"zs_tags\"]:\n",
    "    if col in tmdb.columns:\n",
    "        tag_cols.append(col)\n",
    "\n",
    "if not tag_cols:\n",
    "    print(\"‚ö†Ô∏è No tag columns found; 'all_tags' will be empty lists.\")\n",
    "\n",
    "def _unify_row(row):\n",
    "    bags = []\n",
    "    for c in tag_cols:\n",
    "        bags.extend(_to_list(row.get(c)))\n",
    "    # apply synonyms and dedupe\n",
    "    return sorted(set(_apply_synonyms(bags)))\n",
    "\n",
    "tmdb[\"all_tags\"] = tmdb.apply(_unify_row, axis=1)\n",
    "\n",
    "# 4) Global tag frequency (used by rarity-boost logic in tag_overlap_score)\n",
    "TAG_COUNTS_GLOBAL = Counter(t for tags in tmdb[\"all_tags\"] for t in (tags or []))\n",
    "\n",
    "# 5) Quick visibility\n",
    "print(f\"‚úÖ Tags unified. Movies with ‚â•1 tag: {(tmdb['all_tags'].map(bool).mean()*100):.1f}%\")\n",
    "print(f\"‚úÖ Unique tags in corpus: {len(TAG_COUNTS_GLOBAL):,}\")\n",
    "print(\"   Examples:\", list(TAG_COUNTS_GLOBAL.most_common(5)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31f93d-04c9-4d15-aef6-63ff06dd8aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== FIXED movie_query_parser.py =====\n",
    "# Complete THEME_SYNONYMS mapping all 104 zero-shot labels\n",
    "# Better genre/character extraction\n",
    "\n",
    "import re\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "# Calculate tag rarity for boosting\n",
    "from collections import Counter\n",
    "\n",
    "all_tags = []\n",
    "for tags in tmdb[\"review_tags\"]:\n",
    "    all_tags.extend(tags)\n",
    "\n",
    "TAG_COUNTS_GLOBAL = Counter(all_tags)\n",
    "\n",
    "print(f\"üìä Tag rarity data loaded: {len(TAG_COUNTS_GLOBAL)} unique tags\")\n",
    "print(f\"   Rarest tags (< 50 movies): {sum(1 for c in TAG_COUNTS_GLOBAL.values() if c < 50)}\")\n",
    "\n",
    "# ===== COMPLETE THEME_SYNONYMS (maps natural language ‚Üí zero-shot tags) =====\n",
    "THEME_SYNONYMS = {\n",
    "    # EMOTIONAL / TONAL\n",
    "    \"inspiring\": {\"inspiring\", \"uplifting\", \"hopeful\"},\n",
    "    \"uplifting\": {\"inspiring\", \"uplifting\", \"hopeful\"},\n",
    "    \"feel-good\": {\"feel-good\", \"comforting\", \"family friendly\"},\n",
    "    \"feel good\": {\"feel-good\", \"comforting\", \"family friendly\"},\n",
    "    \"comforting\": {\"comforting\", \"feel-good\"},\n",
    "    \"family friendly\": {\"family friendly\", \"family values\"},\n",
    "    \"family-friendly\": {\"family friendly\", \"family values\"},\n",
    "    \"tragic\": {\"tragic\", \"heartbreaking\", \"bittersweet\"},\n",
    "    \"heartbreaking\": {\"tragic\", \"heartbreaking\"},\n",
    "    \"bittersweet\": {\"bittersweet\", \"lost love\"},\n",
    "    \"lost love\": {\"lost love\", \"romantic\", \"bittersweet\"},\n",
    "    \"romantic\": {\"romantic\", \"slow-burn\"},\n",
    "    \"slow-burn\": {\"slow-burn\", \"romantic\"},\n",
    "    \"dark\": {\"dark\", \"bleak\", \"gritty\"},\n",
    "    \"bleak\": {\"bleak\", \"dark\"},\n",
    "    \"gritty\": {\"gritty\", \"dark\", \"intense\"},\n",
    "    \"mystery\": {\"mystery\", \"suspenseful\"},\n",
    "    \"suspenseful\": {\"suspenseful\", \"thriller\", \"mystery\"},\n",
    "    \"thriller\": {\"thriller\", \"suspenseful\", \"intense\"},\n",
    "    \"documentary\": {\"documentary\"},\n",
    "    \"coming of age\": {\"coming of age\"},\n",
    "    \"coming-of-age\": {\"coming of age\"},\n",
    "    \"redemption\": {\"redemption\", \"overcoming adversity\"},\n",
    "    \"friendship\": {\"friendship\", \"found family\"},\n",
    "    \"found family\": {\"found family\", \"friendship\"},\n",
    "    \"biographical\": {\"biographical\"},\n",
    "    \n",
    "    # SUBJECT / GENRE\n",
    "    \"mafia\": {\"mafia\", \"organized crime\", \"gangster\", \"mob\"},\n",
    "    \"organized crime\": {\"organized crime\", \"crime\", \"mafia\"},\n",
    "    \"gangster\": {\"gangster\", \"mob\", \"crime\"},\n",
    "    \"gangsters\": {\"gangster\", \"mob\", \"crime\"},  # NEW: plural\n",
    "    \"mob\": {\"mob\", \"mafia\", \"gangster\"},\n",
    "    \"crime\": {\"crime\", \"noir\", \"organized crime\"},\n",
    "    \"noir\": {\"noir\", \"crime\"},\n",
    "    \"heist\": {\"heist\", \"crime\"},\n",
    "    \"detective\": {\"detective\", \"mystery\", \"police procedural\"},\n",
    "    \"police procedural\": {\"police procedural\", \"detective\"},\n",
    "    \"courtroom\": {\"courtroom\"},\n",
    "    \"espionage\": {\"espionage\", \"political thriller\"},\n",
    "    \"political thriller\": {\"political thriller\", \"espionage\"},\n",
    "    \n",
    "    # SCI-FI / SPECULATIVE\n",
    "    \"sci-fi\": {\"sci-fi\", \"science fiction\"},\n",
    "    \"science fiction\": {\"science fiction\", \"sci-fi\"},\n",
    "    \"cyberpunk\": {\"cyberpunk\", \"sci-fi\"},\n",
    "    \"dystopian\": {\"dystopian\", \"post-apocalyptic\"},\n",
    "    \"post-apocalyptic\": {\"post-apocalyptic\", \"dystopian\"},\n",
    "    \"time travel\": {\"time travel\", \"sci-fi\"},\n",
    "    \"space opera\": {\"space opera\", \"sci-fi\", \"epic scope\"},\n",
    "    \"alien\": {\"alien\", \"sci-fi\"},\n",
    "    \"robot ai\": {\"robot ai\", \"sci-fi\"},\n",
    "    \"kaiju\": {\"kaiju\", \"sci-fi\"},\n",
    "    \n",
    "    # HORROR / CREATURES\n",
    "    \"vampire\": {\"vampire\", \"horror\"},\n",
    "    \"zombie\": {\"zombie\", \"horror\"},\n",
    "    \"psychological horror\": {\"psychological horror\", \"horror\", \"dark\"},\n",
    "    \"horror\": {\"horror\"},\n",
    "    \n",
    "    # ACTION / MARTIAL / WESTERN\n",
    "    \"superhero\": {\"superhero\", \"action\"},\n",
    "    \"martial arts\": {\"martial arts\", \"action\"},\n",
    "    \"kung fu\": {\"martial arts\", \"action\"},  # NEW!\n",
    "    \"karate\": {\"martial arts\", \"action\"},  # NEW!\n",
    "    \"martial artist\": {\"martial arts\", \"action\"},  # NEW!\n",
    "    \"western\": {\"western\"},\n",
    "    \n",
    "    # DRAMA / ART\n",
    "    \"period drama\": {\"period drama\"},\n",
    "    \"historical epic\": {\"historical epic\", \"epic scope\"},\n",
    "    \"ancient\": {\"historical epic\", \"period drama\"},  # NEW!\n",
    "    \"ancient times\": {\"historical epic\", \"period drama\"},  # NEW!\n",
    "    \"ancient history\": {\"historical epic\", \"period drama\"},  # NEW!\n",
    "    \"medieval\": {\"historical epic\", \"period drama\"},  # NEW!\n",
    "    \"surreal\": {\"surreal\", \"absurdist\"},\n",
    "    \"absurdist\": {\"absurdist\", \"surreal\"},\n",
    "    \"existential\": {\"existential\", \"cerebral\"},\n",
    "    \"arthouse\": {\"arthouse\", \"cerebral\"},\n",
    "    \"satire\": {\"satire\", \"parody\"},\n",
    "    \"parody\": {\"parody\", \"satire\"},\n",
    "    \n",
    "    # FANTASY\n",
    "    \"fantasy\": {\"fantasy\"},\n",
    "    \"sword and sorcery\": {\"sword and sorcery\", \"fantasy\"},\n",
    "    \"myth and legend\": {\"myth and legend\", \"fantasy\"},\n",
    "    \n",
    "    # CHARACTER TYPES\n",
    "    \"strong female lead\": {\"strong female lead\", \"female protagonist\"},\n",
    "    \"female lead\": {\"strong female lead\", \"female protagonist\"},\n",
    "    \"female protagonist\": {\"female protagonist\", \"strong female lead\"},\n",
    "    \"male protagonist\": {\"male protagonist\"},\n",
    "    \"ensemble cast\": {\"ensemble cast\"},\n",
    "    \"anti-hero\": {\"anti-hero\"},\n",
    "    \"underdog\": {\"underdog story\"},\n",
    "    \"underdog story\": {\"underdog story\"},\n",
    "    \"mentor\": {\"mentor relationship\"},\n",
    "    \"father-son\": {\"father-son relationship\"},\n",
    "    \"father\": {\"father-son relationship\"},  # NEW!\n",
    "    \"dad\": {\"father-son relationship\"},  # NEW!\n",
    "    \"son\": {\"father-son relationship\"},  # NEW!\n",
    "    \"boy and his father\": {\"father-son relationship\"},  # NEW!\n",
    "    \"mother-daughter\": {\"mother-daughter relationship\"},\n",
    "    \"mother\": {\"mother-daughter relationship\"},  # NEW!\n",
    "    \"unlikely friendship\": {\"unlikely friendship\"},\n",
    "    \n",
    "    # RELATIONSHIP DYNAMICS\n",
    "    \"forbidden love\": {\"forbidden love\", \"romantic\"},\n",
    "    \"unrequited love\": {\"unrequited love\", \"lost love\"},\n",
    "    \"toxic relationship\": {\"toxic relationship\"},\n",
    "    \"family drama\": {\"family drama\"},\n",
    "    \"generational conflict\": {\"generational conflict\", \"family drama\"},\n",
    "    \"class struggle\": {\"class struggle\"},\n",
    "    \"cultural clash\": {\"cultural clash\"},\n",
    "    \n",
    "    # EMOTIONAL JOURNEY / THEMES\n",
    "    \"revenge\": {\"revenge\", \"justice vs revenge\"},\n",
    "    \"betrayal\": {\"betrayal\"},\n",
    "    \"sacrifice\": {\"sacrifice\"},\n",
    "    \"survival\": {\"survival\"},\n",
    "    \"overcoming adversity\": {\"overcoming adversity\", \"redemption\"},\n",
    "    \"self-discovery\": {\"self-discovery\", \"identity crisis\"},\n",
    "    \"identity crisis\": {\"identity crisis\", \"self-discovery\"},\n",
    "    \"moral dilemma\": {\"moral dilemma\"},\n",
    "    \"loss and grief\": {\"loss and grief\"},\n",
    "    \"grief\": {\"loss and grief\"},\n",
    "    \"hope\": {\"hope\", \"inspiring\"},\n",
    "    \"justice\": {\"justice vs revenge\"},\n",
    "    \"loneliness\": {\"loneliness\"},\n",
    "    \"power and corruption\": {\"power and corruption\"},\n",
    "    \"corruption\": {\"power and corruption\"},\n",
    "    \n",
    "    # PACING / ATMOSPHERE\n",
    "    \"fast-paced\": {\"fast-paced\", \"intense\"},\n",
    "    \"fast paced\": {\"fast-paced\", \"intense\"},\n",
    "    \"intense\": {\"intense\", \"fast-paced\"},\n",
    "    \"contemplative\": {\"contemplative\", \"cerebral\"},\n",
    "    \"whimsical\": {\"whimsical\"},\n",
    "    \"atmospheric\": {\"atmospheric\"},\n",
    "    \"cerebral\": {\"cerebral\", \"contemplative\"},\n",
    "    \n",
    "    # SETTING / SCOPE\n",
    "    \"small-town\": {\"small-town\", \"isolated setting\"},\n",
    "    \"small town\": {\"small-town\", \"isolated setting\"},\n",
    "    \"isolated\": {\"isolated setting\"},\n",
    "    \"road trip\": {\"road trip\"},\n",
    "    \"epic\": {\"epic scope\"},\n",
    "    \"intimate\": {\"intimate story\"},\n",
    "    \n",
    "    # BREAKUPS & RELATIONSHIPS (NEW!)\n",
    "    \"breakup\": {\"lost love\", \"heartbreaking\", \"romantic\"},\n",
    "    \"broke up\": {\"lost love\", \"heartbreaking\", \"romantic\"},\n",
    "    \"broken heart\": {\"lost love\", \"heartbreaking\"},\n",
    "    \"break up\": {\"lost love\", \"heartbreaking\", \"romantic\"},\n",
    "    \n",
    "    # SPORTS (TEMPORARY MAPPING - will improve with V2)\n",
    "    \"sports\": {\"action\"},  # Temp until V2 adds sports tags\n",
    "    \"sport\": {\"action\"},\n",
    "    \"football\": {\"action\"},\n",
    "    \"basketball\": {\"action\"},\n",
    "    \"baseball\": {\"action\"},\n",
    "    \"soccer\": {\"action\"},\n",
    "    \"boxing\": {\"action\"},\n",
    "    \"athletic\": {\"action\"},\n",
    "    \n",
    "    # GENOCIDE / HOLOCAUST (MAPPING TO EXISTING TAGS)\n",
    "    \"genocide\": {\"dark\", \"tragic\", \"historical epic\"},\n",
    "    \"holocaust\": {\"dark\", \"tragic\", \"historical epic\"},\n",
    "    \"war crimes\": {\"dark\", \"tragic\"},\n",
    "}\n",
    "\n",
    "# Avoid false actor matches\n",
    "BAD_ACTOR_WORDS = {\n",
    "    \"dark\", \"psychological\", \"family\", \"inspiring\", \"romantic\", \"sad\", \"happy\",\n",
    "    \"thriller\", \"drama\", \"comedy\", \"action\", \"horror\", \"documentary\"\n",
    "}\n",
    "\n",
    "def query_theme_set(q: str) -> set:\n",
    "    \"\"\"Extract theme tags from query using THEME_SYNONYMS.\"\"\"\n",
    "    ql = q.lower()\n",
    "    out = set()\n",
    "    for kw, tags in THEME_SYNONYMS.items():\n",
    "        if kw in ql:\n",
    "            out |= tags\n",
    "    return out\n",
    "\n",
    "# Emotional intent detection\n",
    "SAD_WORDS = {\"sad\", \"grief\", \"melancholy\", \"heartbroken\", \"lost love\", \"lonely\", \"depressed\", \"depressing\", \"breakup\"}\n",
    "HAPPY_WORDS = {\"feel good\", \"feel-good\", \"happy\", \"uplifting\", \"inspiring\", \"hopeful\", \"joyful\", \"comforting\"}\n",
    "\n",
    "def wants_sad(q: str) -> bool:\n",
    "    ql = q.lower()\n",
    "    return any(kw in ql for kw in SAD_WORDS)\n",
    "\n",
    "def wants_happy(q: str) -> bool:\n",
    "    ql = q.lower()\n",
    "    return any(kw in ql for kw in HAPPY_WORDS)\n",
    "\n",
    "def parse_query_safe(q: str):\n",
    "    \"\"\"\n",
    "    Parse query with BAD_ACTOR_WORDS filter.\n",
    "    Returns filters dict with: year_min, year_max, actor, genres, etc.\n",
    "    \"\"\"\n",
    "    f = parse_query(q)\n",
    "    if \"actor\" in f:\n",
    "        a = str(f[\"actor\"]).lower()\n",
    "        if any(w in a for w in BAD_ACTOR_WORDS):\n",
    "            f.pop(\"actor\", None)\n",
    "    return f\n",
    "\n",
    "# Original parse_query function (placeholder - use your existing one)\n",
    "def parse_query(q: str) -> Dict[str, Any]:\n",
    "    \"\"\"Parse query - extract year, actor, genres.\"\"\"\n",
    "    filters = {}\n",
    "    \n",
    "    # Extract decades (90s, 1990s, etc.) - FIXED VERSION\n",
    "    decade_match = re.search(r\"(\\d{2,4})s\", q)\n",
    "    if decade_match:\n",
    "        decade_str = decade_match.group(1)\n",
    "        try:\n",
    "            if len(decade_str) == 2:\n",
    "                # Handle \"80s\", \"90s\" ‚Üí \"1980\", \"1990\"\n",
    "                decade = int(\"19\" + decade_str)\n",
    "            else:\n",
    "                decade = int(decade_str)\n",
    "            \n",
    "            filters[\"year_min\"] = decade\n",
    "            filters[\"year_max\"] = decade + 9\n",
    "        except ValueError:\n",
    "            # If conversion fails, skip decade filtering\n",
    "            pass\n",
    "    \n",
    "    # Extract explicit years\n",
    "    year_match = re.search(r\"(19|20)\\d{2}\", q)\n",
    "    if year_match:\n",
    "        try:\n",
    "            year = int(year_match.group(0))\n",
    "            filters[\"year_min\"] = year\n",
    "            filters[\"year_max\"] = year\n",
    "        except ValueError:\n",
    "            pass\n",
    "    \n",
    "    # Extract actor names (simple pattern)\n",
    "    # Look for common patterns like \"X movies\" or \"with X\"\n",
    "    actor_patterns = [\n",
    "        r\"([\\w\\s]+?)\\s+movies\",  # \"Bill Murray movies\"\n",
    "        r\"with\\s+([\\w\\s]+?)(?:\\s+from|\\s+in|\\s*$)\",  # \"with Jennifer Lopez\"\n",
    "        r\"starring\\s+([\\w\\s]+?)(?:\\s+from|\\s+in|\\s*$)\",  # \"starring Tom Hanks\"\n",
    "    ]\n",
    "    \n",
    "    for pattern in actor_patterns:\n",
    "        match = re.search(pattern, q, re.IGNORECASE)\n",
    "        if match:\n",
    "            potential_actor = match.group(1).strip()\n",
    "            # Basic validation: 2-4 words, capitalized\n",
    "            words = potential_actor.split()\n",
    "            if 1 <= len(words) <= 4:\n",
    "                filters[\"actor\"] = potential_actor.title()\n",
    "                break\n",
    "    \n",
    "    # Extract genres\n",
    "    genre_keywords = {\n",
    "        \"thriller\": \"Thriller\",\n",
    "        \"drama\": \"Drama\",\n",
    "        \"comedy\": \"Comedy\",\n",
    "        \"action\": \"Action\",\n",
    "        \"horror\": \"Horror\",\n",
    "        \"sci-fi\": \"Science Fiction\",\n",
    "        \"science fiction\": \"Science Fiction\",\n",
    "        \"romance\": \"Romance\",\n",
    "        \"documentary\": \"Documentary\",\n",
    "        \"animation\": \"Animation\",\n",
    "        \"fantasy\": \"Fantasy\",\n",
    "        \"mystery\": \"Mystery\",\n",
    "        \"crime\": \"Crime\",\n",
    "        \"adventure\": \"Adventure\",\n",
    "        \"war\": \"War\",\n",
    "        \"western\": \"Western\"\n",
    "    }\n",
    "    \n",
    "    ql = q.lower()\n",
    "    genres = []\n",
    "    for keyword, genre_name in genre_keywords.items():\n",
    "        if keyword in ql:\n",
    "            genres.append(genre_name)\n",
    "    \n",
    "    if genres:\n",
    "        filters[\"genres\"] = genres\n",
    "    \n",
    "    return filters\n",
    "\n",
    "\n",
    "def filter_by_metadata(df, filters):\n",
    "    \"\"\"Apply metadata filters to dataframe.\"\"\"\n",
    "    result = df.copy()\n",
    "    \n",
    "    # Year filters\n",
    "    if \"year_min\" in filters:\n",
    "        result = result[result[\"year\"] >= filters[\"year_min\"]]\n",
    "    if \"year_max\" in filters:\n",
    "        result = result[result[\"year\"] <= filters[\"year_max\"]]\n",
    "    \n",
    "    # ACTOR FILTERING (NEW IMPLEMENTATION)\n",
    "    if \"actor\" in filters:\n",
    "        actor_name = filters[\"actor\"].lower()\n",
    "        \n",
    "        # Check if 'cast' column exists\n",
    "        if \"cast\" in result.columns:\n",
    "            # Filter by cast column (handles list/string formats)\n",
    "            def has_actor(cast_data):\n",
    "                if pd.isna(cast_data):\n",
    "                    return False\n",
    "                \n",
    "                # Convert to string and check\n",
    "                cast_str = str(cast_data).lower()\n",
    "                return actor_name in cast_str\n",
    "            \n",
    "            result = result[result[\"cast\"].apply(has_actor)]\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è WARNING: 'cast' column not found, cannot filter by actor '{actor_name}'\")\n",
    "    \n",
    "    # Genre filters\n",
    "    if \"genres\" in filters:\n",
    "        req_genres = set(g.lower() for g in filters[\"genres\"])\n",
    "        \n",
    "        def has_genre(genre_list):\n",
    "            if not genre_list:\n",
    "                return False\n",
    "            movie_genres = set(g.lower() for g in genre_list)\n",
    "            return bool(movie_genres & req_genres)\n",
    "        \n",
    "        result = result[result[\"genre_names\"].apply(has_genre)]\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Scoring utilities\n",
    "def tag_overlap_score(tags, q_themes: set):\n",
    "    \"\"\"Score with RARE TAG BOOSTING - rewards specific/niche tags.\"\"\"\n",
    "    tags = set(map(str, (tags or [])))\n",
    "    \n",
    "    # If no query themes, use a neutral score that doesn't dominate\n",
    "    if not q_themes:\n",
    "        return 0.7  # Keep this neutral\n",
    "    \n",
    "    if not tags:\n",
    "        return 0.5\n",
    "    \n",
    "    inter = tags & q_themes\n",
    "    if not inter:\n",
    "        return 0.5\n",
    "    \n",
    "    # Calculate base score\n",
    "    base_score = 0.5 + 0.5 * (len(inter) / max(1, len(tags | q_themes)))\n",
    "    \n",
    "    # BOOST RARE TAGS (the magic fix!)\n",
    "    rarity_boost = 0.0\n",
    "    for tag in inter:\n",
    "        tag_count = TAG_COUNTS_GLOBAL.get(tag, 0)\n",
    "        \n",
    "        if tag_count < 50:  # Ultra-rare (sports, martial arts)\n",
    "            rarity_boost += 0.4\n",
    "        elif tag_count < 200:  # Rare (gangster, western)\n",
    "            rarity_boost += 0.25\n",
    "        elif tag_count < 1000:  # Uncommon (sci-fi, fantasy)\n",
    "            rarity_boost += 0.1\n",
    "        # Common tags (intense, inspiring) get no boost\n",
    "    \n",
    "    final_score = min(1.0, base_score + rarity_boost)\n",
    "    return final_score\n",
    "\n",
    "def soft_genre_score(genres, req_genres):\n",
    "    \"\"\"Score based on genre overlap.\"\"\"\n",
    "    if not req_genres:\n",
    "        return 0.7\n",
    "    g = set(map(str.lower, (genres or [])))\n",
    "    q = set(map(str.lower, req_genres))\n",
    "    if not g:\n",
    "        return 0.7\n",
    "    inter = len(g & q)\n",
    "    uni = len(g | q)\n",
    "    return 0.6 + 0.4 * (inter / max(1, uni))  # 0.6‚Äì1.0\n",
    "\n",
    "def sentiment_match_score(row, q: str):\n",
    "    \"\"\"\n",
    "    Expanded sentiment blending:\n",
    "    - For 'sad/lost love' intents, use sentiment_sad\n",
    "    - For 'feel-good/uplifting' intents, use sentiment_happy\n",
    "    - Otherwise neutral 0.7\n",
    "    \"\"\"\n",
    "    ql = q.lower()\n",
    "    if any(w in ql for w in SAD_WORDS):\n",
    "        val = row.get(\"sentiment_sad\", None)\n",
    "    elif any(w in ql for w in HAPPY_WORDS):\n",
    "        val = row.get(\"sentiment_happy\", None)\n",
    "    else:\n",
    "        return 0.7\n",
    "    \n",
    "    try:\n",
    "        if val is None or (hasattr(val, '__iter__') and len(val) == 0):\n",
    "            return 0.7\n",
    "        import pandas as pd\n",
    "        if pd.isna(val):\n",
    "            return 0.7\n",
    "        return float(0.3 + 0.7 * max(0.0, min(1.0, float(val))))\n",
    "    except Exception:\n",
    "        return 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc45866-13f4-44e9-bbc4-9d93aae5383f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== CompleteRecommender v3.1 - Split Actor Fallback + Bug Fixes =====\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "class CompleteRecommender:\n",
    "    def __init__(self, df, svd, ncf, user_map, movie_map,\n",
    "                 faiss_idx, emb, emb_keys, key2idx, key_set, title_mode=True):\n",
    "        self.df = df\n",
    "        self.svd = svd\n",
    "        self.ncf = ncf\n",
    "        self.user_to_idx = user_map\n",
    "        self.movie_to_idx = movie_map\n",
    "        self.faiss = faiss_idx\n",
    "        self.emb = emb\n",
    "        self.emb_keys = emb_keys\n",
    "        self.key2idx = key2idx\n",
    "        self.key_set = key_set\n",
    "        self.title_mode = title_mode\n",
    "        self.encoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n",
    "    \n",
    "    @staticmethod\n",
    "    def _content(qv, key, key2idx, emb):\n",
    "        idx = key2idx.get(key)\n",
    "        if idx is None:\n",
    "            return 3.0\n",
    "        sim = float(np.dot(qv[0], emb[idx]))\n",
    "        return float(np.clip(1 + 4*((sim+1)/2), 1, 5))\n",
    "    \n",
    "    def _cf(self, user_raw, movie_raw):\n",
    "        if user_raw is None or movie_raw is None:\n",
    "            return 2.5\n",
    "        \n",
    "        try:\n",
    "            svd_pred = self.svd.predict(str(user_raw), str(movie_raw)).est\n",
    "            svd_scaled = float(np.clip(svd_pred, 1, 5))\n",
    "        except Exception:\n",
    "            svd_scaled = 3.0\n",
    "        \n",
    "        uid = self.user_to_idx.get(str(user_raw))\n",
    "        mid = self.movie_to_idx.get(str(movie_raw))\n",
    "        \n",
    "        if uid is not None and mid is not None:\n",
    "            try:\n",
    "                p01 = float(self.ncf.predict([np.array([uid]), np.array([mid])],\n",
    "                                            verbose=0).reshape(-1)[0])\n",
    "                ncf_scaled = float(np.clip(1 + 4*p01, 1, 5))\n",
    "            except Exception:\n",
    "                ncf_scaled = 3.0\n",
    "        else:\n",
    "            ncf_scaled = 3.0\n",
    "        \n",
    "        return 0.5*svd_scaled + 0.5*ncf_scaled\n",
    "    \n",
    "    def _detect_query_type(self, filters, q_themes, query):\n",
    "        \"\"\"Detect query type for adaptive weighting.\"\"\"\n",
    "        has_factual = any(k in filters for k in [\"year_min\", \"year_max\", \"actor\", \"studio\"])\n",
    "        has_semantic = len(q_themes) > 0 or wants_sad(query) or wants_happy(query)\n",
    "        \n",
    "        if has_factual and not has_semantic:\n",
    "            return \"factual\"\n",
    "        elif has_semantic and not has_factual:\n",
    "            return \"semantic\"\n",
    "        else:\n",
    "            return \"mixed\"\n",
    "    \n",
    "    def _get_adaptive_weights(self, query_type, sad_flag, user_id, query):\n",
    "        \"\"\"Return adaptive weights based on query type. FIX: Added query parameter.\"\"\"\n",
    "        if query_type == \"factual\":\n",
    "            if user_id:\n",
    "                return (0.30, 0.50, 0.05, 0.10, 0.05)\n",
    "            else:\n",
    "                return (0.40, 0.30, 0.10, 0.15, 0.05)\n",
    "        \n",
    "        elif query_type == \"semantic\":\n",
    "            if sad_flag or wants_happy(query):\n",
    "                return (0.35, 0.25, 0.20, 0.05, 0.15)\n",
    "            else:\n",
    "                return (0.40, 0.25, 0.25, 0.05, 0.05)\n",
    "        \n",
    "        else:  # mixed\n",
    "            if sad_flag:\n",
    "                return (0.30, 0.30, 0.15, 0.10, 0.15)\n",
    "            else:\n",
    "                return (0.35, 0.35, 0.15, 0.10, 0.05)\n",
    "    \n",
    "    def _score_candidates(self, cand_df, q_themes: set, filters: dict):\n",
    "        \"\"\"\n",
    "        Score candidates with tag/theme relevance (rarity-boosted) first, then quality,\n",
    "        then genre & sentiment nudges. Expects:\n",
    "          - cand_df['all_tags'] list[str]\n",
    "          - cand_df['genre_names'] list[str]\n",
    "          - cand_df['weighted_rating'], ['vote_count'], ['vote_average'] (optional)\n",
    "          - cand_df['sentiment_score'] (optional)\n",
    "        \"\"\"\n",
    "        from movie_query_parser import (\n",
    "            tag_overlap_score, soft_genre_score, sentiment_match_score,\n",
    "            wants_sad, wants_happy\n",
    "        )\n",
    "\n",
    "        # ensure required columns exist\n",
    "        if \"all_tags\" not in cand_df.columns:\n",
    "            cand_df[\"all_tags\"] = [[] for _ in range(len(cand_df))]\n",
    "        if \"genre_names\" not in cand_df.columns:\n",
    "            cand_df[\"genre_names\"] = [[] for _ in range(len(cand_df))]\n",
    "\n",
    "        q_themes = set(q_themes or [])\n",
    "        want_sad = wants_sad(q_themes)\n",
    "        want_happy = wants_happy(q_themes)\n",
    "\n",
    "        scored = []\n",
    "        for _, r in cand_df.iterrows():\n",
    "            # 1Ô∏è‚É£ theme relevance (rarity-boosted via TAG_COUNTS_GLOBAL)\n",
    "            theme = float(tag_overlap_score(r.get(\"all_tags\", []), q_themes))\n",
    "\n",
    "            # 2Ô∏è‚É£ quality signals\n",
    "            wr = float(r.get(\"weighted_rating\", r.get(\"vote_average\", 0.0)) or 0.0) / 10.0\n",
    "            votes = min(1.0, float(r.get(\"vote_count\", 0) or 0.0) / 10000.0)\n",
    "\n",
    "            # 3Ô∏è‚É£ genre & sentiment nudges\n",
    "            genre = float(\n",
    "                soft_genre_score(r.get(\"genre_names\", []), filters.get(\"genres\", []))\n",
    "            ) if filters and \"genres\" in filters else 0.0\n",
    "\n",
    "            sent = float(r.get(\"sentiment_score\", 0.0))\n",
    "            sent_adj = float(\n",
    "                sentiment_match_score(sent, want_sad, want_happy)\n",
    "            ) if q_themes else 0.0\n",
    "\n",
    "            # 4Ô∏è‚É£ weighted blend ‚Äî tag relevance dominates\n",
    "            final = (\n",
    "                0.55 * theme +\n",
    "                0.15 * wr +\n",
    "                0.05 * votes +\n",
    "                0.10 * genre +\n",
    "                0.05 * sent_adj +\n",
    "                0.10 * 0.0    # reserved\n",
    "            )\n",
    "\n",
    "            scored.append({\n",
    "                \"title\": r.get(\"tmdb_title\"),\n",
    "                \"year\": r.get(\"year\"),\n",
    "                \"genres\": r.get(\"genre_names\", []),\n",
    "                \"themes\": r.get(\"all_tags\", []),\n",
    "                \"rating\": r.get(\"vote_average\", 0.0),\n",
    "                \"weighted_rating\": r.get(\"weighted_rating\", 0.0),\n",
    "                \"content\": 0.0, \"cf\": 0.0,\n",
    "                \"theme\": theme,\n",
    "                \"genre_match\": genre,\n",
    "                \"sentiment\": sent_adj,\n",
    "                \"score\": float(final)\n",
    "            })\n",
    "\n",
    "        scored.sort(key=lambda x: x[\"score\"], reverse=True)\n",
    "        return scored\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "    \n",
    "    def recommend(self, query, user_id=None, n=10):\n",
    "        import os\n",
    "        import pandas as pd_local\n",
    "        from pathlib import Path\n",
    "        from movie_query_parser import (\n",
    "            parse_query, parse_query_safe,\n",
    "            filter_by_metadata,\n",
    "            query_theme_set, wants_sad, wants_happy,\n",
    "            tag_overlap_score, soft_genre_score, sentiment_match_score\n",
    "        )\n",
    "\n",
    "        # 1) Parse & encode\n",
    "        filters = parse_query_safe(query)\n",
    "        qv = self.encoder.encode([query]).astype(np.float32)\n",
    "        qv /= np.linalg.norm(qv, axis=1, keepdims=True) + 1e-12\n",
    "        \n",
    "        q_themes = query_theme_set(query)\n",
    "        req_genres = filters.get(\"genres\", [])\n",
    "        sad_flag = wants_sad(query)\n",
    "        \n",
    "        query_type = self._detect_query_type(filters, q_themes, query)\n",
    "        \n",
    "        # 2) FAISS search\n",
    "        D, I = self.faiss.search(qv, min(3000, self.faiss.ntotal))\n",
    "        faiss_keys = [self.emb_keys[i] for i in I[0] if i != -1]\n",
    "        faiss_set = set(faiss_keys)\n",
    "        \n",
    "        # 3) SPLIT ACTOR FALLBACK: Try full query, then split if actor query fails\n",
    "        has_actor = \"actor\" in filters\n",
    "        cand = None\n",
    "        fallback_level = 0\n",
    "        actor_fallback_results = None\n",
    "        \n",
    "        # Level 0: Strict intersection (all filters + FAISS)\n",
    "        meta_df = filter_by_metadata(self.df, filters).copy()\n",
    "        meta_df[\"__key__\"] = meta_df[\"title_norm\"].astype(str)\n",
    "        cand = meta_df[meta_df[\"__key__\"].isin(faiss_set)].copy()\n",
    "        \n",
    "        # SPLIT FALLBACK: If actor query returns <5 results, provide two result sets\n",
    "        if has_actor and len(cand) < 5:\n",
    "            actor_name = filters.get(\"actor\")\n",
    "            \n",
    "            # Result Set A: Drop actor, keep genre/decade/themes\n",
    "            filters_no_actor = {k: v for k, v in filters.items() if k != \"actor\"}\n",
    "            meta_no_actor = filter_by_metadata(self.df, filters_no_actor).copy()\n",
    "            meta_no_actor[\"__key__\"] = meta_no_actor[\"title_norm\"].astype(str)\n",
    "            cand_no_actor = meta_no_actor[meta_no_actor[\"__key__\"].isin(faiss_set)].copy()\n",
    "            \n",
    "            # Result Set B: Keep actor, drop other constraints\n",
    "            filters_actor_only = {\"actor\": actor_name}\n",
    "            if \"year_min\" in filters:\n",
    "                filters_actor_only[\"year_min\"] = filters[\"year_min\"]\n",
    "            if \"year_max\" in filters:\n",
    "                filters_actor_only[\"year_max\"] = filters[\"year_max\"]\n",
    "            \n",
    "            meta_actor_only = filter_by_metadata(self.df, filters_actor_only).copy()\n",
    "            meta_actor_only[\"__key__\"] = meta_actor_only[\"title_norm\"].astype(str)\n",
    "            cand_actor_only = meta_actor_only.head(500).copy()\n",
    "            \n",
    "            if len(cand_no_actor) > 0:\n",
    "                cand = cand_no_actor.head(1200).copy()\n",
    "                fallback_level = \"split_primary\"\n",
    "                \n",
    "                # Score actor fallback separately\n",
    "                if len(cand_actor_only) > 0:\n",
    "                    content_w, cf_w, tag_w, genre_w, sentiment_w = self._get_adaptive_weights(\n",
    "                        \"factual\", sad_flag, user_id, query  # Actor-only is factual\n",
    "                    )\n",
    "                    \n",
    "                    actor_fallback_results = self._score_candidates(\n",
    "                        cand_actor_only.head(500), qv, user_id, query, q_themes, [],\n",
    "                        0.30, 0.50, 0.05, 0.05, 0.10, \"split_actor\", \"factual\"\n",
    "                    )\n",
    "            else:\n",
    "                cand = cand_actor_only\n",
    "                fallback_level = \"actor_only\"\n",
    "        \n",
    "        # Continue with normal fallback if still empty\n",
    "        if len(cand) == 0 and len(meta_df) > 0:\n",
    "            cand = meta_df.head(500).copy()\n",
    "            fallback_level = 1\n",
    "        \n",
    "        if len(cand) == 0:\n",
    "            temp_df = self.df.copy()\n",
    "            temp_df[\"__key__\"] = temp_df[\"title_norm\"].astype(str)\n",
    "            cand = temp_df[temp_df[\"__key__\"].isin(faiss_set)].head(500).copy()\n",
    "            fallback_level = 2\n",
    "        \n",
    "        if len(cand) == 0:\n",
    "            cand = self.df.nlargest(500, \"weighted_rating\").copy()\n",
    "            cand[\"__key__\"] = cand[\"title_norm\"].astype(str)\n",
    "            fallback_level = 3\n",
    "        \n",
    "        if len(cand) == 0:\n",
    "            return []\n",
    "        \n",
    "        cand = cand.head(1200).copy()\n",
    "        \n",
    "        # Get adaptive weights\n",
    "        content_w, cf_w, tag_w, genre_w, sentiment_w = self._get_adaptive_weights(\n",
    "            query_type, sad_flag, user_id, query\n",
    "        )\n",
    "        \n",
    "        # 4) Score primary candidates\n",
    "        out = self._score_candidates(\n",
    "            cand, qv, user_id, query, q_themes, req_genres,\n",
    "            content_w, cf_w, tag_w, genre_w, sentiment_w, fallback_level, query_type\n",
    "        )\n",
    "        \n",
    "        # 5) Re-ranker (optional)\n",
    "        ART = Path(r\"C:\\Users\\kylek\\artifacts\").resolve()\n",
    "        RE_RANKER_PATH = ART / \"re_ranker_lgb.txt\"\n",
    "        \n",
    "        def apply_reranker(results):\n",
    "            if os.path.exists(RE_RANKER_PATH) and len(results) > 0:\n",
    "                import lightgbm as lgb\n",
    "                booster = lgb.Booster(model_file=str(RE_RANKER_PATH))\n",
    "                rX = pd_local.DataFrame([{\n",
    "                    \"content\": r[\"content\"],\n",
    "                    \"cf\": r[\"cf\"],\n",
    "                    \"tag_score\": r.get(\"theme\", 0.7),\n",
    "                    \"weighted_rating\": (r.get(\"weighted_rating\", 5.0) or 5.0) / 10.0\n",
    "                } for r in results])\n",
    "                \n",
    "                rerank_scores = booster.predict(rX)\n",
    "                for r, s in zip(results, rerank_scores):\n",
    "                    r[\"rerank\"] = float(s)\n",
    "                \n",
    "                results.sort(key=lambda x: x.get(\"rerank\", -1.0), reverse=True)\n",
    "            else:\n",
    "                results.sort(key=lambda x: (x[\"score\"], x.get(\"weighted_rating\", 0.0)), reverse=True)\n",
    "            return results\n",
    "        \n",
    "        out = apply_reranker(out)\n",
    "        \n",
    "        # Apply reranker to actor fallback too\n",
    "        if actor_fallback_results:\n",
    "            actor_fallback_results = apply_reranker(actor_fallback_results)\n",
    "        \n",
    "        # Return format: If split happened, return dict with both sets\n",
    "        if actor_fallback_results:\n",
    "            return {\n",
    "                \"primary\": out[:n],\n",
    "                \"actor_fallback\": actor_fallback_results[:n],\n",
    "                \"split_query\": True,\n",
    "                \"actor_name\": filters.get(\"actor\")\n",
    "            }\n",
    "        else:\n",
    "            return out[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30ae3b5-8c8e-404e-bc0c-acaa4beaffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== COMBINED: Instantiation + Testing (All-in-One Cell) =====\n",
    "\n",
    "# ============================================================================\n",
    "# PART 1: INSTANTIATE RECOMMENDER\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üöÄ Instantiating CompleteRecommender...\")\n",
    "\n",
    "recommender = CompleteRecommender(\n",
    "    df=tmdb,\n",
    "    svd=svd_model,\n",
    "    ncf=ncf_model,\n",
    "    user_map=user_to_idx,\n",
    "    movie_map=movie_to_idx,\n",
    "    faiss_idx=faiss_index,\n",
    "    emb=emb,\n",
    "    emb_keys=emb_keys,      # ‚Üê CRITICAL for v3.1\n",
    "    key2idx=key2idx,\n",
    "    key_set=key_set,\n",
    "    title_mode=TITLE_MODE\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Recommender ready (Master v4 - v3.1)\")\n",
    "print(f\"   ‚Ä¢ Loaded {len(tmdb):,} movies\")\n",
    "print(f\"   ‚Ä¢ {len(user_to_idx):,} users, {len(movie_to_idx):,} CF IDs\")\n",
    "\n",
    "# ============================================================================\n",
    "# PART 2: PRETTY_PRINT FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def pretty_print(results):\n",
    "    \"\"\"Updated to handle split actor fallback format.\"\"\"\n",
    "    \n",
    "    # Check if split format (actor query with fallback)\n",
    "    if isinstance(results, dict) and \"split_query\" in results:\n",
    "        actor_name = results.get(\"actor_name\", \"actor\")\n",
    "        primary = results.get(\"primary\", [])\n",
    "        actor_fallback = results.get(\"actor_fallback\", [])\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"üìå SPLIT RESULTS (exact match not found)\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        print(f\"\\nüé¨ PRIMARY: Matching genre/themes/decade (without {actor_name}):\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        if not primary:\n",
    "            print(\"   No results\")\n",
    "        else:\n",
    "            for i, r in enumerate(primary, 1):\n",
    "                g = \", \".join(r[\"genres\"][:3]) if isinstance(r[\"genres\"], list) else \"\"\n",
    "                print(f\"{i:2d}. {r['title'][:55]:55s} ({r['year']}) ‚≠ê{r['rating']:.1f} | WR={r['weighted_rating']:.2f}\")\n",
    "                if g:\n",
    "                    print(f\"    üé≠ {g}\")\n",
    "                if r.get(\"themes\"):\n",
    "                    print(f\"    üè∑Ô∏è  {', '.join(map(str, r['themes'][:3]))}\")\n",
    "                print(f\"    üìä Content={r['content']:.2f} CF={r['cf']:.2f} Theme={r.get('theme', 0.7):.2f} Sentiment={r.get('sentiment', 0.7):.2f}\")\n",
    "                print()\n",
    "        \n",
    "        print(f\"\\nüë§ ACTOR FALLBACK: All {actor_name} movies from specified time:\")\n",
    "        print(f\"{'-'*80}\")\n",
    "        if not actor_fallback:\n",
    "            print(\"   No results\")\n",
    "        else:\n",
    "            for i, r in enumerate(actor_fallback, 1):\n",
    "                g = \", \".join(r[\"genres\"][:3]) if isinstance(r[\"genres\"], list) else \"\"\n",
    "                print(f\"{i:2d}. {r['title'][:55]:55s} ({r['year']}) ‚≠ê{r['rating']:.1f} | WR={r['weighted_rating']:.2f}\")\n",
    "                if g:\n",
    "                    print(f\"    üé≠ {g}\")\n",
    "                if r.get(\"themes\"):\n",
    "                    print(f\"    üè∑Ô∏è  {', '.join(map(str, r['themes'][:3]))}\")\n",
    "                print(f\"    üìä Content={r['content']:.2f} CF={r['cf']:.2f} Theme={r.get('theme', 0.7):.2f} Sentiment={r.get('sentiment', 0.7):.2f}\")\n",
    "                print()\n",
    "        \n",
    "        return\n",
    "    \n",
    "    # Normal format (list of movies)\n",
    "    if not results:\n",
    "        print(\"‚ùå No results\")\n",
    "        return\n",
    "    \n",
    "    for i, r in enumerate(results, 1):\n",
    "        g = \", \".join(r[\"genres\"][:3]) if isinstance(r[\"genres\"], list) else \"\"\n",
    "        print(f\"{i:2d}. {r['title'][:55]:55s} ({r['year']}) ‚≠ê{r['rating']:.1f} | WR={r['weighted_rating']:.2f}\")\n",
    "        if g:\n",
    "            print(f\"    üé≠ {g}\")\n",
    "        if r.get(\"themes\"):\n",
    "            print(f\"    üè∑Ô∏è  {', '.join(map(str, r['themes'][:3]))}\")\n",
    "        print(f\"    üìä Content={r['content']:.2f} CF={r['cf']:.2f} Theme={r.get('theme', 0.7):.2f} Sentiment={r.get('sentiment', 0.7):.2f}\")\n",
    "        print()\n",
    "\n",
    "# ============================================================================\n",
    "# PART 3: TEST QUERIES\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üß™ RUNNING TEST QUERIES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test query list\n",
    "queries = [\n",
    "    \"Bill Murray movies from the 90s rated above 7\",\n",
    "    \"like Breaking Bad but with a female lead\",\n",
    "    \"an inspiring, family-friendly drama\",\n",
    "    \"dark psychological thriller from the 2010s\",\n",
    "    \"I'm sad and want a movie about lost love\",\n",
    "    \"dark psychological thriller with Jennifer Lopez from the 90s\"  # Tests split fallback\n",
    "]\n",
    "\n",
    "# Run each query\n",
    "for q in queries:\n",
    "    print(f\"\\n--- Query: {q} ---\")\n",
    "    try:\n",
    "        results = recommender.recommend(q, n=10)\n",
    "        pretty_print(results)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ERROR: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"‚úÖ All tests complete!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97c36a-5c7f-4851-8fc4-0cda5d7ca66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this in a new cell - show me the output\n",
    "print(\"=== DIAGNOSTIC CHECK ===\")\n",
    "print(f\"1. THEME_SYNONYMS exists: {'THEME_SYNONYMS' in dir()}\")\n",
    "if 'THEME_SYNONYMS' in dir():\n",
    "    print(f\"   Length: {len(THEME_SYNONYMS)}\")\n",
    "else:\n",
    "    print(\"   ‚ùå NOT LOADED\")\n",
    "\n",
    "print(f\"\\n2. emb_keys exists: {'emb_keys' in dir()}\")\n",
    "if 'emb_keys' in dir():\n",
    "    print(f\"   Type: {type(emb_keys)}\")\n",
    "    print(f\"   Length: {len(emb_keys)}\")\n",
    "else:\n",
    "    print(\"   ‚ùå NOT CREATED\")\n",
    "\n",
    "print(f\"\\n3. CompleteRecommender class:\")\n",
    "print(f\"   Has _get_adaptive_weights: {hasattr(recommender, '_get_adaptive_weights')}\")\n",
    "print(f\"   Has _score_candidates: {hasattr(recommender, '_score_candidates')}\")\n",
    "\n",
    "# Check parser function\n",
    "from movie_query_parser import parse_query_safe\n",
    "test_parse = parse_query_safe(\"Bill Murray movies from the 90s\")\n",
    "print(f\"\\n4. Parser test: {test_parse}\")\n",
    "\n",
    "# Check theme extraction\n",
    "from movie_query_parser import query_theme_set\n",
    "test_themes = query_theme_set(\"dark psychological thriller\")\n",
    "print(f\"\\n5. Theme extraction test: {test_themes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb95d64-817b-4415-99b5-46668480b46f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Cell D: Gradio Interface (OPTIONAL) =====\n",
    "# Place AFTER Cell C if you want the interactive UI\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def recommend_ui(query, user_id, top_n):\n",
    "    if not query.strip():\n",
    "        return \"Please enter a query\"\n",
    "    \n",
    "    user = user_id.strip() if user_id.strip() else None\n",
    "    results = recommender.recommend(query, user_id=user, n=int(top_n))\n",
    "    \n",
    "    if not results:\n",
    "        return \"No results found\"\n",
    "    \n",
    "    output = []\n",
    "    for i, r in enumerate(results, 1):\n",
    "        genres = \", \".join(r[\"genres\"][:3]) if r[\"genres\"] else \"N/A\"\n",
    "        themes = \", \".join(map(str, r[\"themes\"][:3])) if r[\"themes\"] else \"N/A\"\n",
    "        output.append(\n",
    "            f\"{i}. **{r['title']}** ({r['year']})\\n\"\n",
    "            f\"   ‚≠ê {r['rating']:.1f} | WR: {r['weighted_rating']:.2f}\\n\"\n",
    "            f\"   üé≠ {genres}\\n\"\n",
    "            f\"   üè∑Ô∏è {themes}\\n\"\n",
    "        )\n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "demo = gr.Interface(\n",
    "    fn=recommend_ui,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Query\", placeholder=\"Type a query (natural language), optionally add a training user ID, choose Top N, then click Submit.\"),\n",
    "        gr.Textbox(label=\"User ID (optional)\", placeholder=\"\"),\n",
    "        gr.Slider(1, 20, value=10, step=1, label=\"Top N\")\n",
    "    ],\n",
    "    outputs=gr.Textbox(label=\"Results\"),\n",
    "    title=\"Hybrid Movie Recommender\",\n",
    "    description=\"Type a query (natural language), optionally add a training user ID, choose Top N, then click Submit.\",\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "demo.launch(inline=True, height=520, show_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "388814c4-d6bd-4e6b-b16c-2c489e517c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check what tags are actually being used\n",
    "from collections import Counter\n",
    "\n",
    "# Get all tags across all movies\n",
    "all_tags = []\n",
    "for tags in tmdb[\"review_tags\"]:\n",
    "    all_tags.extend(tags)\n",
    "\n",
    "tag_counts = Counter(all_tags)\n",
    "\n",
    "print(\"üîù Top 30 most common tags:\")\n",
    "for tag, count in tag_counts.most_common(30):\n",
    "    print(f\"  {tag:30s} ‚Üí {count:,} movies\")\n",
    "\n",
    "print(f\"\\nüìä Total unique tags: {len(tag_counts)}\")\n",
    "\n",
    "# Check specific tags we care about\n",
    "print(\"\\nüéØ Specific tag coverage:\")\n",
    "important_tags = [\n",
    "    \"martial arts\", \"sports\", \"kung fu\", \"boxing\",\n",
    "    \"ancient rome\", \"ancient greece\", \"medieval\", \"historical epic\",\n",
    "    \"gangster\", \"mob\", \"mafia\", \"organized crime\",\n",
    "    \"father-son relationship\", \"mother-daughter relationship\"\n",
    "]\n",
    "\n",
    "for tag in important_tags:\n",
    "    count = tag_counts.get(tag, 0)\n",
    "    print(f\"  {tag:30s} ‚Üí {count:,} movies\")\n",
    "\n",
    "# Check if Enter the Dragon exists\n",
    "print(\"\\nü•ã Checking for famous martial arts movies:\")\n",
    "martial_arts_movies = [\"enter the dragon\", \"ip man\", \"crouching tiger hidden dragon\", \n",
    "                       \"the raid\", \"ong bak\", \"kung fu panda\"]\n",
    "\n",
    "for title in martial_arts_movies:\n",
    "    matches = tmdb[tmdb[\"title_norm\"].str.contains(title, na=False)]\n",
    "    if len(matches) > 0:\n",
    "        movie = matches.iloc[0]\n",
    "        print(f\"  {movie['tmdb_title']:40s} ‚Üí Tags: {movie['review_tags']}\")\n",
    "    else:\n",
    "        print(f\"  '{title}' ‚Üí NOT IN DATASET\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86995471-2794-4c10-9d8e-3016abcb4262",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if famous movies are tagged correctly\n",
    "famous_checks = {\n",
    "    \"goodfellas\": \"gangster\",\n",
    "    \"the godfather\": \"gangster\",\n",
    "    \"gladiator\": \"historical epic\",\n",
    "    \"300\": \"historical epic\",\n",
    "    \"rocky\": \"sports\",\n",
    "    \"hoosiers\": \"sports\",\n",
    "    \"enter the dragon\": \"martial arts\"\n",
    "}\n",
    "\n",
    "for title_search, expected_tag in famous_checks.items():\n",
    "    matches = tmdb[tmdb[\"title_norm\"].str.contains(title_search, na=False)]\n",
    "    if len(matches) > 0:\n",
    "        movie = matches.iloc[0]\n",
    "        has_tag = expected_tag in movie[\"review_tags\"]\n",
    "        symbol = \"‚úÖ\" if has_tag else \"‚ùå\"\n",
    "        print(f\"{symbol} {movie['tmdb_title']:40s} ‚Üí {expected_tag:20s} ‚Üí {movie['review_tags'][:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e23f211-a174-4820-94e8-b8cbb4bcfc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== AMERICAN PIE CHECK ===\")\n",
    "american_pie = tmdb[tmdb['tmdb_title'].str.contains('American Pie', case=False, na=False)]\n",
    "print(f\"Movies matching 'American Pie': {len(american_pie)}\")\n",
    "if len(american_pie) > 0:\n",
    "    for _, row in american_pie.iterrows():\n",
    "        print(f\"  - {row['tmdb_title']} ({row.get('year')})\")\n",
    "        print(f\"    Cast: {row.get('cast', 'NO CAST DATA')[:200]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4419ef-971d-4baf-9fe8-c7c30885bbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FAISS INDEX CHECK ===\")\n",
    "\n",
    "# Check if American Pie is in the embeddings\n",
    "test_titles = [\"american pie\", \"american pie 1999\", \"american pie 2001\"]\n",
    "\n",
    "for title in test_titles:\n",
    "    normalized = norm_title(title)  # Use your norm_title function\n",
    "    print(f\"\\nSearching for: '{title}' ‚Üí normalized: '{normalized}'\")\n",
    "    \n",
    "    if normalized in key_set:\n",
    "        idx = key2idx[normalized]\n",
    "        print(f\"  ‚úÖ FOUND in FAISS at index {idx}\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå NOT FOUND in FAISS index\")\n",
    "\n",
    "# Check what's actually in emb_keys around \"american\"\n",
    "print(f\"\\nüìã Sample of titles starting with 'american' in FAISS:\")\n",
    "american_titles = [k for k in emb_keys[:10000] if k.startswith('american')]\n",
    "for t in american_titles[:10]:\n",
    "    print(f\"  - {t}\")\n",
    "\n",
    "print(f\"\\nTotal 'american' titles in FAISS: {len(american_titles)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e92a1c-8dc3-4910-88db-84fe2e6b582c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA STRUCTURE MISMATCH CHECK ===\\n\")\n",
    "\n",
    "print(f\"1. key_set size: {len(key_set):,}\")\n",
    "print(f\"2. key2idx size: {len(key2idx):,}\")\n",
    "print(f\"3. emb_keys size: {len(emb_keys):,}\")\n",
    "print(f\"4. FAISS index size: {faiss_index.ntotal:,}\")\n",
    "\n",
    "print(f\"\\n5. Are sizes consistent?\")\n",
    "print(f\"   key_set == key2idx: {len(key_set) == len(key2idx)}\")\n",
    "print(f\"   key2idx == emb_keys: {len(key2idx) == len(emb_keys)}\")\n",
    "print(f\"   emb_keys == FAISS: {len(emb_keys) == faiss_index.ntotal}\")\n",
    "\n",
    "# Check what emb_keys actually contains\n",
    "print(f\"\\n6. First 10 emb_keys:\")\n",
    "for i, key in enumerate(emb_keys[:10]):\n",
    "    print(f\"   [{i}] {key}\")\n",
    "\n",
    "# Check if emb_keys has the same entry at index 14564\n",
    "if len(emb_keys) > 14564:\n",
    "    print(f\"\\n7. emb_keys[14564] = '{emb_keys[14564]}'\")\n",
    "else:\n",
    "    print(f\"\\n7. ‚ùå emb_keys only has {len(emb_keys)} entries, can't access index 14564\")\n",
    "\n",
    "# Check tmdb vs key_set overlap\n",
    "tmdb_titles_normalized = tmdb['tmdb_title'].apply(norm_title)\n",
    "overlap = sum(1 for t in tmdb_titles_normalized if t in key_set)\n",
    "print(f\"\\n8. tmdb titles in key_set: {overlap:,} / {len(tmdb):,} ({100*overlap/len(tmdb):.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8d3a5c-071d-4202-b5a4-dafcec6ad7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
